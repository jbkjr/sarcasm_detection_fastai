{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ftfy\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"\n",
    "    Return set of symbol pairs in a word.\n",
    "    word is represented as tuple of symbols (symbols being variable-length strings)\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "def text_standardize(text):\n",
    "    \"\"\"\n",
    "    fixes some issues the spacy tokenizer had on books corpus\n",
    "    also does some whitespace standardization\n",
    "    \"\"\"\n",
    "    text = text.replace('—', '-')\n",
    "    text = text.replace('–', '-')\n",
    "    text = text.replace('―', '-')\n",
    "    text = text.replace('…', '...')\n",
    "    text = text.replace('´', \"'\")\n",
    "    text = re.sub(r'''(-+|~+|!+|\"+|;+|\\?+|\\++|,+|\\)+|\\(+|\\\\+|\\/+|\\*+|\\[+|\\]+|}+|{+|\\|+|_+)''', r' \\1 ', text)\n",
    "    text = re.sub(r'\\s*\\n\\s*', ' \\n ', text)\n",
    "    text = re.sub(r'[^\\S\\n]+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "pre_rules = [ftfy.fix_text, text_standardize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(object):\n",
    "    \"\"\"\n",
    "    mostly a wrapper for a public python bpe tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_path, bpe_path):\n",
    "        self.nlp = spacy.load('en', disable=['parser', 'tagger', 'ner', 'textcat'])\n",
    "        self.encoder = json.load(open(encoder_path))\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        merges = open(bpe_path, encoding='utf-8').read().split('\\n')[1:-1]\n",
    "        merges = [tuple(merge.split()) for merge in merges]\n",
    "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "    def bpe(self, token):\n",
    "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token+'</w>'\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        if word == '\\n  </w>':\n",
    "            word = '\\n</w>'\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, texts, verbose=True):\n",
    "        texts_tokens = []\n",
    "        if verbose:\n",
    "            for text in tqdm(texts, ncols=80, leave=False):\n",
    "                text = self.nlp(text_standardize(ftfy.fix_text(text)))\n",
    "                text_tokens = []\n",
    "                for token in text:\n",
    "                    text_tokens.extend([self.encoder.get(t, 0) for t in self.bpe(token.text.lower()).split(' ')])\n",
    "                texts_tokens.append(text_tokens)\n",
    "        else:\n",
    "            for text in texts:\n",
    "                text = self.nlp(text_standardize(ftfy.fix_text(text)))\n",
    "                text_tokens = []\n",
    "                for token in text:\n",
    "                    text_tokens.extend([self.encoder.get(t, 0) for t in self.bpe(token.text.lower()).split(' ')])\n",
    "                texts_tokens.append(text_tokens)\n",
    "        return texts_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder('models/encoder_bpe_40000.json', 'models/vocab_40000.bpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe(toks, text_encoder):\n",
    "    text_tokens = []\n",
    "    for tok in toks[1:]:\n",
    "        text_tokens.extend(text_encoder.bpe(tok.lower()).split(' '))\n",
    "    return [BOS] + text_tokens + ['_classify_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_rules = [partial(bpe, text_encoder=text_encoder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(pre_rules=pre_rules, post_rules=post_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = text_encoder.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = [k for k,v in stoi.items()]\n",
    "itos.insert(0, '_classify_')\n",
    "itos.insert(0, BOS)\n",
    "itos.insert(0, UNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk', 'xxbos', '_classify_', '.', ',', 't', 'h', 'e', '\"', 'o']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_lm = TextLMDataBunch.from_csv(path, 'Sarcasm_Headlines_Dataset.csv')\n",
    "data_clas = TextClasDataBunch.from_csv(path, 'Sarcasm_Headlines_Dataset.csv', \n",
    "                                       tokenizer = tokenizer, vocab = vocab, bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>'&lt;/w&gt; 12&lt;/w&gt; years&lt;/w&gt; a&lt;/w&gt; slave&lt;/w&gt; ,&lt;/w&gt; '&lt;/w&gt; '&lt;/w&gt; captain&lt;/w&gt; phillips&lt;/w&gt; ,&lt;/w&gt; '&lt;/w&gt; '&lt;/w&gt; american&lt;/w&gt; hustle&lt;/w&gt; ,&lt;/w&gt; '&lt;/w&gt; '&lt;/w&gt; wolf&lt;/w&gt; of&lt;/w&gt; wall&lt;/w&gt; street&lt;/w&gt; ,&lt;/w&gt; '&lt;/w&gt; '&lt;/w&gt; blue&lt;/w&gt; jasmine&lt;/w&gt; ,&lt;/w&gt; '&lt;/w&gt; '&lt;/w&gt; dallas&lt;/w&gt; buyers&lt;/w&gt; club&lt;/w&gt; ,&lt;/w&gt; '&lt;/w&gt; '&lt;/w&gt; her&lt;/w&gt; ,&lt;/w&gt; '&lt;/w&gt; '&lt;/w&gt; nebraska&lt;/w&gt; ,&lt;/w&gt; '&lt;/w&gt; '&lt;/w&gt; before&lt;/w&gt; midnight&lt;/w&gt; ,&lt;/w&gt; '&lt;/w&gt; and&lt;/w&gt; '&lt;/w&gt; philo men a&lt;/w&gt; '&lt;/w&gt; all&lt;/w&gt; written&lt;/w&gt; during&lt;/w&gt; same&lt;/w&gt; continuing&lt;/w&gt; education&lt;/w&gt; screen writing&lt;/w&gt; class&lt;/w&gt; _classify_</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>maya&lt;/w&gt; angel ou&lt;/w&gt; ,&lt;/w&gt; poet&lt;/w&gt; ,&lt;/w&gt; author&lt;/w&gt; ,&lt;/w&gt; civil&lt;/w&gt; rights&lt;/w&gt; acti vist&lt;/w&gt; ,&lt;/w&gt; and&lt;/w&gt; -&lt;/w&gt; holy&lt;/w&gt; cow&lt;/w&gt; -&lt;/w&gt; tony&lt;/w&gt; award&lt;/w&gt; -&lt;/w&gt; nominated&lt;/w&gt; actress&lt;/w&gt; ,&lt;/w&gt; college&lt;/w&gt; professor&lt;/w&gt; ,&lt;/w&gt; magazine&lt;/w&gt; editor&lt;/w&gt; ,&lt;/w&gt; street car&lt;/w&gt; conductor&lt;/w&gt; -&lt;/w&gt; really&lt;/w&gt; ?&lt;/w&gt; street car&lt;/w&gt; conductor&lt;/w&gt; ?&lt;/w&gt; wow&lt;/w&gt; -&lt;/w&gt; calypso&lt;/w&gt; singer&lt;/w&gt; ,&lt;/w&gt; nightclub&lt;/w&gt; performer&lt;/w&gt; ,&lt;/w&gt; and&lt;/w&gt; foreign&lt;/w&gt; journalist&lt;/w&gt; ,&lt;/w&gt; dead&lt;/w&gt; at&lt;/w&gt; 86&lt;/w&gt; _classify_</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>occasionally&lt;/w&gt; you&lt;/w&gt; realize&lt;/w&gt; someone&lt;/w&gt; you&lt;/w&gt; thought&lt;/w&gt; was&lt;/w&gt; a&lt;/w&gt; dear&lt;/w&gt; friend&lt;/w&gt; is&lt;/w&gt; actually&lt;/w&gt; a&lt;/w&gt; foe&lt;/w&gt; ,&lt;/w&gt; their&lt;/w&gt; true&lt;/w&gt; character&lt;/w&gt; finally&lt;/w&gt; revealed&lt;/w&gt; .&lt;/w&gt; but&lt;/w&gt; how&lt;/w&gt; do&lt;/w&gt; you&lt;/w&gt; forgive&lt;/w&gt; the&lt;/w&gt; unforgivable&lt;/w&gt; ?&lt;/w&gt; here&lt;/w&gt; are&lt;/w&gt; my&lt;/w&gt; 10&lt;/w&gt; steps&lt;/w&gt; to&lt;/w&gt; handling&lt;/w&gt; betrayal&lt;/w&gt; with&lt;/w&gt; elegance&lt;/w&gt; and&lt;/w&gt; grace&lt;/w&gt; .&lt;/w&gt; _classify_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>navy&lt;/w&gt; dis continues&lt;/w&gt; use&lt;/w&gt; of&lt;/w&gt; '&lt;/w&gt; port&lt;/w&gt; '&lt;/w&gt; and&lt;/w&gt; '&lt;/w&gt; star boar d' will&lt;/w&gt; now&lt;/w&gt; refer&lt;/w&gt; to&lt;/w&gt; left&lt;/w&gt; as&lt;/w&gt; '&lt;/w&gt; thunk&lt;/w&gt; '&lt;/w&gt; and&lt;/w&gt; right&lt;/w&gt; as&lt;/w&gt; '&lt;/w&gt; moo sh&lt;/w&gt; -&lt;/w&gt; bar oo&lt;/w&gt; '&lt;/w&gt; _classify_</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>a&lt;/w&gt; labor&lt;/w&gt; day&lt;/w&gt; documentary&lt;/w&gt; :&lt;/w&gt; '&lt;/w&gt; brothers&lt;/w&gt; on&lt;/w&gt; the&lt;/w&gt; line&lt;/w&gt; '&lt;/w&gt; tells&lt;/w&gt; the&lt;/w&gt; story&lt;/w&gt; of&lt;/w&gt; the&lt;/w&gt; reu ther&lt;/w&gt; brothers&lt;/w&gt; --&lt;/w&gt; founding&lt;/w&gt; fathers&lt;/w&gt; of&lt;/w&gt; the&lt;/w&gt; american&lt;/w&gt; middle&lt;/w&gt; class&lt;/w&gt; _classify_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clas, Transformer, drop_mult=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 02:11 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>0.490509</th>\n",
       "    <th>0.515282</th>\n",
       "    <th>0.767690</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>0.427520</th>\n",
       "    <th>0.516817</th>\n",
       "    <th>0.777237</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>0.372072</th>\n",
       "    <th>0.395693</th>\n",
       "    <th>0.822538</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(3, 6.25e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fai",
   "language": "python",
   "name": "fai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
